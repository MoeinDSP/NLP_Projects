{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Agentic AI\n",
        "In this notebook we will make use of the LangChain toolkit (https://python.langchain.com/) to develop various LLM powered applications."
      ],
      "metadata": {
        "id": "mJTrwIDFSnPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing LangChain and Configuring an LLM\n",
        "\n",
        "First let's install LangChain:"
      ],
      "metadata": {
        "id": "CeOhzdFpwjFc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hegj7SISmcS"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain needs an LLM to run, and thus we will need to provide it with an endpoint where it can access an LLM.\n",
        "This endpoint could be either:\n",
        "- a commercial API, such as the ones of OpenAI, Anthropic, Google, etc.\n",
        "- or by an opensource LLM (e.g. Llama3.2) that is run locally using the deployment library Ollama.\n",
        "\n",
        "We will try using a commercial API to start with. Google provides a free tier version of its Gemini API that we can use, so we'll try that one.\n",
        "- Let's first install the required library from LangChain to access Google's GenAI:"
      ],
      "metadata": {
        "id": "aKoJRGxCYtV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "id": "jQNGINQndPr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To access Google's GenAI API you will need an API key:\n",
        "- To get a key, go to Google's API Studio (https://aistudio.google.com/) and create a new API key.\n",
        "- Once you have it, copy the key into the code snippet below."
      ],
      "metadata": {
        "id": "EKjfdG5AePQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY=\"...\""
      ],
      "metadata": {
        "id": "PxHmWrX9aF-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create a Gemini2 LLM instance by connecting to the API as follows:"
      ],
      "metadata": {
        "id": "9rzDW0dYdsxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=API_KEY)"
      ],
      "metadata": {
        "id": "4LhCgwSZUS5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a quick look at the model:"
      ],
      "metadata": {
        "id": "tyepH4kkeCEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "id": "07wwNG3eeHNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well that doesn't tell us much. It's not an open-source model, so Google won't tell us exactly what architecture they are using anyway ...\n",
        "\n",
        "If you look at the \"gemini-2.0-flash\" model here (https://ai.google.dev/gemini-api/docs/models?hl=en#gemini-2.0-flash), we see that the model:  \n",
        "- can read in a document of up to a million tokens in length (which is a lot!), and produce an ouput of up to 8 thousand tokens;\n",
        "- can take as input also audio, images and video, as well as text;\n",
        "- was trained on data up to August 2024.\n",
        "\n",
        "We can start to chat with the LLM as follows:"
      ],
      "metadata": {
        "id": "Xoc3igzHeSat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.invoke(\"What are the capitals of Australia, Canada, Brasil and South Africa?\").content)"
      ],
      "metadata": {
        "id": "eCBBOefheKhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phew! It got those right at least ...\n",
        "\n",
        "We can see the entire message returned by the model along with the metadata as follows:"
      ],
      "metadata": {
        "id": "7dU7R0AZiNSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"What are the capitals of Australia, Canada, Brasil and South Africa?\")"
      ],
      "metadata": {
        "id": "ATL3WeIJ3s4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More importantly, we can see how funny this model is ..."
      ],
      "metadata": {
        "id": "vualLrtc3vUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Tell me a funny joke about Google, OpenAI and a big bag of bananas.\").content"
      ],
      "metadata": {
        "id": "CqiopiFdeb0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These LLMs are not very funny ...\n",
        "\n",
        "To make the LLM more useful, we'll need to prompt the model using a proper chat template. In LangChain we can do that by defining a prompts as follows:"
      ],
      "metadata": {
        "id": "XWaBn_XzfmnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Pretend you're a drunken pirate, with a predilection to speak in rhyme and include the term \\\"whippersnapper\\\" frequently in conversation.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "ygg-u_hEemOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then define a pipeline, called a \"chain\" in LangChain, using the pipe operator \"|\" as follows:"
      ],
      "metadata": {
        "id": "hCptneJBgQQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "VF6E7m8kgTWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now instead of invoking the LLM directly, we can invoke the chain, which will apply the appropriate prompt that we defined above:"
      ],
      "metadata": {
        "id": "DUumrabJgl-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"input\": \"Tell me about the time you caught a whale with a fishing rod.\"}).content\n",
        "print(response)"
      ],
      "metadata": {
        "id": "u5eyYTrTggbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, that was interesting. What if we ask a more serious question:"
      ],
      "metadata": {
        "id": "41sOzdi_Bn8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"input\": \"When was your database last updated?\"}).content\n",
        "print(response)"
      ],
      "metadata": {
        "id": "6-ZmzGqflkEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like we might need to change that system prompt before we try to use this LLM for serious applications ..."
      ],
      "metadata": {
        "id": "RB2-760dhGPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\"You are a helpful assistant. Answer all questions to the best of your ability.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "dvvUyzVwgy-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check that the prompt has been updated:"
      ],
      "metadata": {
        "id": "s_hKIbG3nnAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"input\": \"When was your database last updated?\"}).content\n",
        "print(response)"
      ],
      "metadata": {
        "id": "VDm-Hu9InTCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That looks more reasonable ;-)"
      ],
      "metadata": {
        "id": "rWvgjFCvmMs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introducing Tools\n",
        "\n",
        "The reason for using LangChain is to build applications on top of LLMs.\n",
        "- To make these applications useful, they will need to access additional resources (referred to as *tools* in LangChain) that they can use to solve problems.\n",
        "- We'll now motivate the need for such tools with a simple example. Consider the following request to the LLM:"
      ],
      "metadata": {
        "id": "8nGQ_2DFtTvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"input\": \"At an annual interest rate of 95% interest, what would 50 dollars be worth in 10 years time?\"}).content\n",
        "print(response)"
      ],
      "metadata": {
        "id": "YyonRwR6Fb6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, that looks good. Let's just do a quick check to make sure that it got the answer right:"
      ],
      "metadata": {
        "id": "v7r6gm-ywCW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"$50 after 10 years at 95% interest rate would be: \",50*((1 + 95/100)**10))"
      ],
      "metadata": {
        "id": "Bv378KlHvcxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oh no! The LLM had the right calculation, but didn't calculate the right value. The value had the right order of magnitude, but was off by quite a large percentage. This is because the model didn't have access to a calculator to perform the exponentiation calculation.\n",
        "\n",
        "Let's see if we can help the model by giving it access to some **tools** for computing the compound interest formula.\n",
        "- A tool in LangChain is simply a method for calculating something.\n",
        "- Tools are just computations of some form that can be implemented in Python code.\n",
        "- (See the LangChain documentation here for more information: https://python.langchain.com/docs/how_to/tools_prompting/)\n",
        "\n",
        "Let's define two tools (functions that the model might make use of) that perform the compound interest calculation and the exponentiation calculation, which is difficult for an LLM to learn how to do automatically:"
      ],
      "metadata": {
        "id": "QwBO8yfcwXEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def compound_interest(principal: float, rate: float, years: float) -> float:\n",
        "   \"\"\"\n",
        "   Computes result of applying particular interest rate to a principal for a number of years.\n",
        "   First argument is the principal, second is the rate, and the third is the number of years.\n",
        "   Equation calculated is: output = principal * (1+rate/100)^years.\n",
        "   \"\"\"\n",
        "   return principal * ((1+rate/100) ** years)\n",
        "\n",
        "@tool\n",
        "def exponentiate(base: float, power: float) -> float:\n",
        "   \"\"\"Computes base^power, i.e. raises the first argument to the power of the second argument.\"\"\"\n",
        "   return base ** power"
      ],
      "metadata": {
        "id": "lT1xxer_tb3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the:\n",
        "- the use of the \"**@tool**\" keyword (known as a 'decorator') before the definition of the function,\n",
        "- the specification of the **types** of the input arguments and returned value from the function,\n",
        "- the addition of a textual description **explaining** what operation is performed by the function.\n",
        "\n",
        "Once we have defined the tools, we can check that they function as desired by directly invoking them:\n"
      ],
      "metadata": {
        "id": "_iuf-MwZxeBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(compound_interest.invoke({\"principal\": 50, \"rate\": 95, \"years\": 10}))\n",
        "\n",
        "print(exponentiate.invoke({\"base\": 1.95, \"power\": 10}))"
      ],
      "metadata": {
        "id": "Lw0uaHxDxOc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which was the value we saw before, when we calculated the power directly.\n",
        "\n",
        "Now in order for the LLM to make use of the tools, we need to tell it about them.\n",
        "- First thing, let's get a list of the available tools and print out their specications:"
      ],
      "metadata": {
        "id": "uqotXWkiyxqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [compound_interest, exponentiate]\n",
        "\n",
        "# Let's inspect the tools\n",
        "for t in tools:\n",
        "    print(\"--\")\n",
        "    print(t.name)\n",
        "    print(t.description)\n",
        "    print(t.args)"
      ],
      "metadata": {
        "id": "gGGypaNauTdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain provides a specific format for printing tool information. Let's try it:"
      ],
      "metadata": {
        "id": "IAFkcWaqd84K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import render_text_description\n",
        "\n",
        "rendered_tools = render_text_description(tools)\n",
        "print(rendered_tools)"
      ],
      "metadata": {
        "id": "4D5m12stuFMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now in order for the LLM to know that the tools are available:\n",
        "- We will need to let it know about their existance and how to use them in the prompt.\n",
        "- We can do that as follows:"
      ],
      "metadata": {
        "id": "MPffqLD3eK8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"\\\n",
        "You are an assistant that has access to the following set of tools.\n",
        "Here are the names and descriptions for each tool:\n",
        "\n",
        "{rendered_tools}\n",
        "\n",
        "Given the user input, return the name and input of the tool to use.\n",
        "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
        "\n",
        "The `arguments` should be a dictionary, with keys corresponding\n",
        "to the argument names and the values corresponding to the requested values.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eU9xHk5euarF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to update the chain with the new prompt:"
      ],
      "metadata": {
        "id": "fVV62BF6jioG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
        ")\n",
        "\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "jpnvgYs9jP6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's invoke the chain to see if anything has changed:"
      ],
      "metadata": {
        "id": "03pJ0LgIjyYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"input\": \"At an annual interest rate of 95% interest, what would 50 dollars be worth in 10 years time?\"}).content\n",
        "print(response)"
      ],
      "metadata": {
        "id": "C0GG20JuJ4FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, that certainly looks different now!\n",
        "- Instead of returning text, the model has returned a JSON object that calls one of the tools we provided!\n",
        "- Note that the LLM hasn't actually called the tool. It has just performed the task we told it to do, which is to format a request to the tool.\n",
        "- In order to make use of this new structured output, we'll need some code to parse the JSON object. We can simply add a JSON parser to the end of our chain:\n",
        "\n"
      ],
      "metadata": {
        "id": "Nec49-NrkMBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "chain_with_parser = prompt | llm | JsonOutputParser()\n",
        "chain_with_parser.invoke({\"input\": \"At an annual interest rate of 95% interest, what would 50 dollars be worth in 10 years time?\"})"
      ],
      "metadata": {
        "id": "s_DfMjD4vmWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great. We now have the parsed JSON object, but we still need to invoke the tool that is specified in it.\n",
        "- To do this we will need some code that can can extract the tool name and arguments from the object and call the method associated with a tool.\n",
        "- This can be done with some Python code as follows:"
      ],
      "metadata": {
        "id": "OU_6FVKqlfSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Dict, Optional, TypedDict\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "class ToolCallRequest(TypedDict):\n",
        "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
        "    name: str\n",
        "    arguments: Dict[str, Any]\n",
        "\n",
        "def invoke_tool(tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None):\n",
        "    \"\"\"A function that we can use the perform a tool invocation.\n",
        "\n",
        "    Args:\n",
        "        tool_call_request: a dict that contains the keys name and arguments.\n",
        "            The name must match the name of a tool that exists.\n",
        "            The arguments are the arguments to that tool.\n",
        "        config: This is configuration information that LangChain uses that contains\n",
        "            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.\n",
        "\n",
        "    Returns:\n",
        "        output from the requested tool\n",
        "    \"\"\"\n",
        "    tool_name_to_tool = {tool.name: tool for tool in tools}\n",
        "    name = tool_call_request[\"name\"]\n",
        "    requested_tool = tool_name_to_tool[name]\n",
        "    return requested_tool.invoke(tool_call_request[\"arguments\"], config=config)"
      ],
      "metadata": {
        "id": "UHYZRqufzVRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, so we now have a method called \"invoke_tool\" that can invoke the tool requested in the JSON object.\n",
        "- Let's add it to the chain:"
      ],
      "metadata": {
        "id": "rVJSoDi5mWnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_parser_and_invoker = prompt | llm | JsonOutputParser() | invoke_tool"
      ],
      "metadata": {
        "id": "b-qK_sxcm1Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_parser_and_invoker.invoke({\"input\": \"At an annual interest rate of 95% interest, what would 50 dollars be worth in 10 years time?\"})"
      ],
      "metadata": {
        "id": "9Z9XzSDA1ZL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few, that worked!\n",
        "\n",
        "How cool is that!?\n",
        "- The LLM has decided it needed to use one of the methods we had provided it in order to answer the question,\n",
        "- and in using it (with a little JSON plumbing help from us), it was able to produce the correct value.\n",
        "\n",
        "Oftentimes in real applications, we wouldn't stop there, but would want the conversation with the chatbot to continue.\n",
        "- That would involve adding the output of the tool to the conversation, so that the chatbot could make use of it in responding directly to the user.\n",
        "- We won't set up the whole conversation for the moment (updates to the LangChain API have made that easy to do recently, so we might return to them at the end of the tutorial).\n",
        "- For the moment we'll show that it's possible to pass the arguements to the function through the chain along with the output of the function call, so that all the information could be added to a prompt if we wanted it. We can pass the informatoin using a different method from LangChain:"
      ],
      "metadata": {
        "id": "01yoQ-WH1v9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain2 = prompt | llm | JsonOutputParser() | RunnablePassthrough.assign(output=invoke_tool)"
      ],
      "metadata": {
        "id": "6djzhq9o13zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, let's try iit with a different request this time. One that ought make use of the other tool we provided:"
      ],
      "metadata": {
        "id": "lIVNwcrao6no"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain2.invoke({\"input\": \"What would the number 1.95 raised to the power of 10 be?\"})"
      ],
      "metadata": {
        "id": "ldb03z0ro7T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that output contains the arguments to the function as well as the output, so that would could, if we wanted to, use this information directly in a prompt (along with the original question from the user) to generate a new output from the LLM.\n",
        "\n",
        "Note that this type of interaction is exactly what is needed to make a RAG (Retrieval Augmented Generative) model, by creating a tool that can perform search."
      ],
      "metadata": {
        "id": "p5gWy3Lfon-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Controlling a Radio\n",
        "\n",
        "OK, that was a lot of fun, but in real applications these tools can be used for far more than just performing maths calculations. They can even be used to make changes to the \"real world\", such as to insert new information into a database.\n",
        "\n",
        "Let's see if we can set up an application in which we try to control another program: In particular, let's try to control a radio that is streaming content from the web.\n",
        "- First to set up an internet radio player, we create an HTML page and add an 'audio' element to it.\n",
        "- Then to enable the radio to be controlled programatically, we can add a *script* that listens for commands on a 'broadcast' channel as follows:  "
      ],
      "metadata": {
        "id": "YqQq4zpzyv-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "stream_url = \"https://stream3.thesocalsound.org/1\"\n",
        "\n",
        "HTML(f\"\"\"\n",
        "\n",
        "<audio id=\"player\" controls autoplay>\n",
        "  <source src=\"{stream_url}\" type=\"audio/mpeg\">\n",
        "  Your browser does not support the audio element.\n",
        "</audio>\n",
        "\n",
        "<script>\n",
        "  var player = document.getElementById('player');\n",
        "  // control player from other cells by sending broadcast message\n",
        "  const listenerChannel = new BroadcastChannel('channel');\n",
        "  listenerChannel.onmessage = (msg) => {{\n",
        "    if (msg.data === 'play') {{ player.play();}}\n",
        "    else if (msg.data === 'pause') {{ player.pause(); }}\n",
        "    else if (msg.data === 'increase volume') {{ player.volume = player.volume + 0.3; }}\n",
        "    else if (msg.data === 'decrease volume') {{ player.volume = player.volume - 0.3; }}\n",
        "  }};\n",
        "</script>\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "dd5B_ZkcqZ-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can control the radio remotely (i.e. programatically from another Python cell) by posting appropriate messages on the broadcast channel using Javascript.\n",
        "- Try uncommenting different messages below to change the command being sent to the radio player:"
      ],
      "metadata": {
        "id": "7GlTOkAkhsgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "\n",
        "message = 'pause'\n",
        "#message = 'play'\n",
        "#message = 'increase volume'\n",
        "#message = 'decrease volume'\n",
        "\n",
        "display(Javascript(f\"\"\"(new BroadcastChannel('channel')).postMessage('{message}');\"\"\"))"
      ],
      "metadata": {
        "id": "T1D479PriOvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use this remote control functionality to define a set of tools:"
      ],
      "metadata": {
        "id": "l5h9WhKqhLVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def play_radio_tool():\n",
        "  \"starts the radio\"\n",
        "  display(Javascript(f\"\"\"(new BroadcastChannel('channel')).postMessage('play');\"\"\"))\n",
        "\n",
        "@tool\n",
        "def pause_radio_tool():\n",
        "  \"pauses the radio\"\n",
        "  display(Javascript(f\"\"\"(new BroadcastChannel('channel')).postMessage('pause');\"\"\"))\n",
        "\n",
        "@tool\n",
        "def increase_radio_volume_tool():\n",
        "  \"increases the radio volume\"\n",
        "  display(Javascript(f\"\"\"(new BroadcastChannel('channel')).postMessage('increase volume');\"\"\"))\n",
        "\n",
        "@tool\n",
        "def decrease_radio_volume_tool():\n",
        "  \"decreases the radio volume\"\n",
        "  display(Javascript(f\"\"\"(new BroadcastChannel('channel')).postMessage('decrease volume');\"\"\"))\n"
      ],
      "metadata": {
        "id": "xmfDIc3L_QDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add these new tools to the set of tools available to the LLM by updating the system prompt to include their description:"
      ],
      "metadata": {
        "id": "QO3q2OawYtGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [exponentiate, compound_interest, play_radio_tool, pause_radio_tool, increase_radio_volume_tool, decrease_radio_volume_tool]\n",
        "rendered_tools = render_text_description(tools)\n",
        "\n",
        "system_prompt = f\"\"\"\\\n",
        "You are an assistant that has access to the following set of tools.\n",
        "Here are the names and descriptions for each tool:\n",
        "\n",
        "{rendered_tools}\n",
        "\n",
        "Given the user input, return the name and input of the tool to use.\n",
        "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
        "\n",
        "The `arguments` should be a dictionary, with keys corresponding\n",
        "to the argument names and the values corresponding to the requested values.\n",
        "\n",
        "If the tool takes no arguments, provide an empty dictionary.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
        ")\n",
        "\n",
        "chain_with_parser_and_invoker = prompt | llm | JsonOutputParser() | RunnablePassthrough.assign(output=invoke_tool)"
      ],
      "metadata": {
        "id": "b9t6SGWvRJLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that since these new tools don't actually take any arguments, we have added another line to the prompt to tell the LLM what to do if the tool doesn't take any arguments:\n",
        "- *'If the tool takes no arguments, provide an empty dictionary.'*\n",
        "- This was necessary to prevent errors being produced by the JSON parser when no dictionary was provided by the LLM.\n",
        "\n",
        "And now let's see if the LLM is capable of understanding instructions to control the radio.\n",
        "- Can it turn the radio on?"
      ],
      "metadata": {
        "id": "YkRu5KCTLtZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_parser_and_invoker.invoke({\"input\": \"Hey, can you turn the radio on?\"})"
      ],
      "metadata": {
        "id": "cjVKa1NrSGAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow, that worked! Now let's turn it off:"
      ],
      "metadata": {
        "id": "3WKBxCU2Nqc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_parser_and_invoker.invoke({\"input\": \"Hey, the radio volume is too high, can you turn it down please? Thanks.\"})"
      ],
      "metadata": {
        "id": "-KQqZlszSYC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Too cool!!\n",
        "The LLM is performing the Natural Language Understanding (NLU) task of converting commands in natural language into calls to specific actions (traditionally called *'Frames'* in NLP).\n",
        "\n",
        "Let's see if the model can turn the volume up and down:"
      ],
      "metadata": {
        "id": "G84kmgHaN4Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_parser_and_invoker.invoke({\"input\": \"Now I can hardly hear it. Please turn up the radio.\"})"
      ],
      "metadata": {
        "id": "Dgt3QXihYe4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_parser_and_invoker.invoke({\"input\": \"Hey, stop that radio. It's terrible!\"})"
      ],
      "metadata": {
        "id": "HpW_XP8UUkkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a play around with the text to see whether the model is robust to different ways of requesting the same activity.\n",
        "\n",
        "If we integrated the above functionality with a speech-to-text model, we would have a voice-interactive system for controlling the radio."
      ],
      "metadata": {
        "id": "Paed9si6OvqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Many more things to try!\n",
        "\n",
        "There are lots of different directions for exploring LangChain functionality:\n",
        "- Use Google New's queryable RSS feed (*'https://news.google.com/rss/search?hl=en-US&q='* + topic) to create a news-headline search tool, that is then used by an LLM (in a RAG model) to provide users with news updates on a particular topic.\n",
        "- One very interesting idea is to create a ReAct (Reasoning Action) agent, which uses an updated version of LangChain to (see https://python.langchain.com/docs/how_to/chatbots_tools/) to easily integrate the results of tool invocations into the conversation."
      ],
      "metadata": {
        "id": "FteC253LysvS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FmOaHbN4L5Dr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}